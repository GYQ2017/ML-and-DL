### 支持向量机

- 时间：2018-08-27
- 摘要：首先讲述`SVM`的基本概念，然后介绍最流行的一种实现－`SMO算法` (求解支持向量机二次规划)，最后介绍一下核函数
- 优缺点：泛化错误率低，计算开销不大，结果易解释；但是对参数调节和核函数的选择敏感

#### 概述

将数据集分隔开的对象，被称为超平面，即分类的决策边界。我们希望找到离分隔超平面最近的点，确保他们离分隔面的距离尽可能远，这里点到分隔面的距离被称为__间隔__ 。

__支持向量__就是离分隔超平面最近的那些点，于是试着最大化`支持向量`到分隔面的距离，需要找到此问题的优化求解方法

- 寻找最大间隔

分隔超平面的形式可写成 $w^Tx+b$ ，要计算点A到分隔超平面的距离，就必须给出点到分隔面的法线或垂线的长度。其中 $label*(w^Tx+b)$ 为点到分隔面的函数间隔，$label*(w^Tx+b)*\frac{1}{||w||}$ 为点到分隔面的几何间隔

- 分类器优化问题

当计算数据点到分隔面的距离并确定分隔面的放置位置时，间隔通过 $label*(w^Tx+b)$ 来计算，现在的目标就是找出分类器定义的`w`和`b` ，为此，我们必须找到具有最小间隔的数据点(支持向量)，一旦找到具有最小间隔的数据点，需要对该间隔进行最大化
$$
arg\max_{w,b}\left\{\min_n(label*w^Tx+b)*\frac{1}{||w||} \right\}
$$
在上述优化问题中，给定了一些约束条件然后求最优值，这里的约束条件就是$label*(w^Tx+b)\geq1.0$ 。对于这类问题，有一个非常著名的求解方法，即拉格朗日乘子法。通过引入拉格朗日乘子，我们就可以基于约束条件来表述原来的问题，于是优化目标函数最后写成
$$
\max_{\alpha}\left[\sum_{i=1}^{m}\alpha-\frac{1}{2}\sum_{i,j=1}^{m}label^{i}\cdot label^{j}\cdot \alpha_i\cdot \alpha_j \left<x^{(i)},x^{(j)}\right> \right]
$$
其约束条件为$\alpha \geq0$ 和$\sum_{i=1}^{m}\alpha_i\cdot label^{(i)}=0$ 

#### SMO优化算法

算法目标是求出一系列`alpha`和`b`，一旦求出这些`alpha`，很容易计算出权重向量`w`，并得到超平面

算法工作原理是：每次循环中选择两个`alpha`进行优化处理，一旦找到一对合适的`alpha`，那么就增大其中一个同时减小另一个。是因为我们有一个约束条件$\sum\alpha_i \cdot label^{(i)}=0$ 

SMO伪代码如下：

```
创建一个alpha向量并将其初始化为0向量
当迭代次数小于最大迭代次数时：
	对数据集中的每个数据向量：
		如果该数据向量可以被优化：
			随机选择另一个数据向量
			同时优化这两个向量
			如果两个向量都不能被优化，退出此次循环
	如果所有向量都没被初始化，增加迭代数目，继续下一次循环
```

#### 核函数

对于非线性的数据，`SVM`的处理方法是选择一个核函数，通过将数据映射到高维空间，来解决原始空间中线性不可分的问题。

具体来说，在线性不可分的情况下，支持向量机首先在__低维空间中完成计算__，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开