### R-NET 模型

- 时间: 2018-09-07
- 摘要: 

#### R-NET 概述

R-NET 一种用于__阅读理解__风格问答的端到端神经网络模型，旨在回答特定段落的问题

我们首先使用基于门控注意力的循环网络将问题和段落进行匹配，获得问题感知的段落表示；然后提出一种自我匹配注意力机制，通过将文章与其自身进行匹配来细化表示，这有效地编码了来自整个文章的信息；最终使用指针网络来定位段落中答案的位置

#### 四个内容

1. `循环网络编码器` 分别构建问题和段落的表示
2. `门控匹配层` 用于匹配问题和段落
3. `自匹配层` 用于汇总整个段落信息
4. 基于`指针网络` 的答案边界预测层

#### 关键贡献

1. 提出了一个基于__`注意力的门控循环网络`__，它为基于注意力的循环网络增加了一个门。事实上，段落中的单词对于回答阅读理解和问答的特定问题具有不同的重要性，通过引入门控机制，基于注意力的门控循环网络根据与问题的相关性，分配不同的权重；
2. 引入了一种__`自匹配机制`__，可以有效地从整篇文章中收集证据来推断答案。但是循环网络只能记住有限的段落上下文，一个答案候选人通常不知道该段落其他部分的线索。为了解决这个问题，我们提出了一个__自匹配层__，利用来自整个段落的信息来动态优化段落表示。基于问题意识的段落表示，我们在段落中使用基于注意力的门控循环网络来对抗文章本身，收集与当前段落相关的证据
3. 提升了结果

#### R-NET 结构

首先，问题和段落分别由双向循环网络处理；然后我们使用基于注意力的门控循环网络匹配问题和段落，以获得段落的问题感知表示；最重要的是，我们应用`自匹配注意力` 去从整个段落收集证据，并细化段落表示，然后输入到输出层去预测答案边界

![09_R-NET 模型](/home/gaoyinquan/MS/Deep Learning/DL_Picture/09_R-NET 模型.png)

1. 问题和段落编码

   我们首先将单词转换为它们各自的单词级嵌入(Glove)$\left\{e_t^Q\right\}_{t=1}^m$和$\left\{ e_t^P \right\}_{t=1}^n$，字符级嵌入(char embedding)$\left\{ c_t^Q \right\}_{t=1}^m$和$\left\{ c_t^P \right\}_{t=1}^n$。字符级嵌入是通过将双向循环网络 (RNN) 的最终隐藏状态应用于特征中的字符嵌入而生成的，分别是问题和段落的所有单词。最终把段落P和问题Q用双向RNN进行encoding得到新表示$u_1^Q,....u_m^Q$和$u_1^P,....u_n^P$ 
   $$
   u_t^Q = BiRNN_Q(u_{t-1}^Q,[e_t^Q,c_t^Q])
   $$

   $$
   u_t^P = BiRNN_P(u_{t-1}^P,[e_t^P,c_t^P])
   $$

2. 基于注意力的门控循环网络

   我们提出了一个基于注意力的门控循环网络，用以把问题信息$u_t^Q$加入到段落表示$u_t^P$中，得到问题Q意识下的段落表示$v_t^P$。这是基于注意力的循环网络的变体，有一个额外的门来确定文章中关于问题的信息的重要性。在给定问题和段落表示的情况下，通过问题和段落中单词的软对齐来生成“句子对”表示$(v_t^P)_{t=1}^n$ 
   $$
   v_t^P = RNN(v_{t-1}^P,c_t)
   $$
   其中$c_t=att(u^Q,[u_t^P,v_{t-1}^P])$ 是对问题$u^Q$ 的一个`注意力池化`向量: $c_t = \sum_{i=1}^m a_t^i u_i^Q$ 
   $$
   a_t^i = exp(s_i^t)/\sum_{j=1}^m exp(s_j^t)
   $$

   $$
   s_j^t  = v^T tanh(W_u^Qu_j^Q+W_u^Pu^P_t+W_v^Pv_{t-1}^P)
   $$

   每个段落表示$v_t^P$ 动态的合并来自整个问题的匹配信息，在match-LSTM中，$v_t^p$作为额外的输入到循环网络中: $v_t^P = RNN(v_{t-1}^P,[u_t^P,c_t])$ 

   为了确定段落部分的重要性并关注与问题相关的部分，我们在RNN输入$([u_t^p,c_t])$中添加了另一个门:
   $$
   g_t = sigmoid(W_g[u_t^P,c_t])
   $$

   $$
   [u_t^P,c_t]^*= g_t \odot [u_t^P,c_t]
   $$

   这额外的门是基于当前段落词和问题的注意力池化向量，其侧重于问题与当前段落词之间的关系。门有效的模拟了只有段落的某些部分与DBQA相关的现象。$[u_t^P,c_t]^*$ 用于后续计算

3. 自匹配注意机制

   通过基于门控注意的循环网络，产生了问题感知段落表示$(v_t^p)_{t=1}^n$ ，以精确定位段落中的重要部分。这种表示一个问题是，它对上下文的了解非常有限。一个答案候选人常常忽略了周围窗口外段落中的重要线索。段落上下文对推理答案很重要，为了解决这个问题，我们直接将问题感知段落表示与自身进行匹配。它从整个段落中动态收集段落中单词的证据，并将与当前段落单词及其匹配问题信息相关的证据编码到段落表示$h_t^P$中: $h_t^P=BiRNN(h_{t-1}^P,[v_t^P,c_t])$ ，其中$c_t=att(v^P,v_t^P)$是基于当前单词下整篇段落的表示。
   $$
   c_t = \sum_{i=1}^n a_i^tv_i^P
   $$

   $$
   a_i^t = exp(s_i^t)/\sum_{j=1}^nexp(s^t_j)
   $$

   $$
   s_j^t = v^T tanh(W_v^Pv^P_j+W_vPv^\tilde{P}_t)
   $$

   自我匹配根据当前的段落词和问题信息从整个段落中提取证据。

4. 输出层

   使用`指针网络(pointer networks)`去预测答案的开始和结束。除此之外，我们在问题表示上使用注意池来为指针网络生成初始隐藏向量。给定段落表示$(h_t^P)_{t=1}^n$，注意力机制被用作指针来从段落中选择开始位置($P^1$)和结束位置($P^2$)，其可以表述如下:
   $$
   s_j^t = v^Ttanh(W_h^Ph^P_j+W_h^ah_{t-1}^a)
   $$

   $$
   a_i^t = exp(s^t_i)/\sum_{j=1}^n exp(s_j^t)
   $$

   $$
   P^t = argmax(a_1^t,...,a_n^t)
   $$

   $h_{t-1}^a$表示pointer network的最后隐藏状态。这指针网络的输入是基于当前预测概率$a^t$ 的注意池向量: 
   $$
   c_t = \sum_{i=1}^na_i^th_i^P
   $$

   $$
   h_t^a = RNN(h_{t-1}^a,c_t)
   $$

   当预测开始位置时，$h_{t-1}^a$ 表示指针网络的初始隐藏状态，我们使用问题向量$r^Q$ 作为指针网络的初始状态。$r^Q$是一个基于段落$V_r^Q$ 的注意池向量:
   $$
   s_j = v^T tanh(W^Q_uu_j^Q+W_v^QV_r^Q)
   $$

   $$
   a_i = exp(s_i)/\sum_{j=1}^mexp(s_j)
   $$

   $$
   r^Q = \sum_{i=1}^m a_iu_i^Q
   $$

   为了训练网络，我们通过预测的概率分布最小化真实开始和结束位置的负对数概率之和。

#### 主要流程

![09_R-NET](/home/gaoyinquan/MS/Deep Learning/DL_Picture/09_R-NET.png)

#### 实验

1. SQuAD v1.1数据集介绍



2. 

#### 参考文献

1. [R-NET机器阅读理解（原理解析）](https://blog.csdn.net/jyh764790374/article/details/80247204) 
2. [R-NET 实现-HKUST](https://github.com/HKUST-KnowComp/R-Net) 
3. [R-NET实现-NLPLearn](https://github.com/NLPLearn/R-net) 
4. [SQuAD 综述](https://mp.weixin.qq.com/s/aZ5o7WPQkne9UflMro0zZg) 
