## Tensorflow学习笔记

基于Keith的[博客](https://blog.csdn.net/column/details/13300.html?&page=3)，个人整理而成的笔记

###  笔记一 命令行参数

主要是在全局环境下编写代码，通过命名行的方式输入参数

~~~python
import tensorflow as tf
# flags是一个文件：flags.py，用于处理命令行参数的解析工作
flags = tf.flags 
logging = tf.logging

#调用flags内部的DEFINE_string函数来制定解析规则
flags.DEFINE_string("para_name_1","default_val", "description")
flags.DEFINE_bool("para_name_2","default_val", "description")

#FLAGS是一个对象，保存了解析后的命令行参数
FLAGS = flags.FLAGS

def main(_):
    FLAGS.para_name #调用命令行输入的参数

# 使用这种方式保证了，如果此文件被其它文件import的时候，不会执行main中的代码
if __name__ == "__main__": 
    tf.app.run() #解析命令行参数，调用main函数 main(sys.argv)
~~~

### 笔记二 tensor操作

- 主要是对矩阵进行系列操作，如__降维__、__连接__、__划分__、__填充__、__升维__

### 笔记八 Dropout

用来防止过拟合，一般用在全连接部分，分为普通dropout和RNN中的dropout

~~~python
lstm_cell=tf.nn.rnn_cell.BasicLSTMCell(size,forget_bias=0.0,state_is_tuple=True)lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=0.5)
~~~

### 笔记十 sess.run()

- TensorFlow编码方法是：先构图，再训练。即总是先定义好整个图，然后才调用`sess.run()`，但只是计算了与想要fetch的值相关的部分

~~~python
import tensorflow as tf
state = tf.Variable(0.0,dtype=tf.float32)
one = tf.constant(1.0,dtype=tf.float32)
new_val = tf.add(state, one)
update = tf.assign(state, new_val)
init = tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)
    for _ in range(3):
        print sess.run(update)   #output is 1.0,2.0,3.0
~~~

~~~python
import tensorflow as tf
y = tf.Variable(1)
b = tf.identity(y)
with tf.Session() as sess:
    tf.global_variables_initializer().run()
    print(sess.run(b,feed_dict={y:3})) #output is 3
    print(sess.run(b))  #output is 1
~~~

### 笔记三十六 自动减少学习率

训练神经网络时，开始我们使用较大的learning rate，随着训练进行，我们慢慢减小learning rate，提供了`API`，即`tf.train.exponential_decay`

更新公式为:

```python
decayed_learning_rate = learning_rate*decay_rate^(global_step/decay_steps)
```

~~~python
import tensorflow as tf 
global_step = tf.Variable(0, trainable=False)
initial_learning_rate = 0.1 #初始学习率
learning_rate = tf.train.exponential_decay(initial_learning_rate,
                                           global_step=global_step,
                                           decay_steps=10,decay_rate=0.9)
'''
与下面相同
opt = tf.train.GradientDescentOptimizer(learning_rate)
add_global = global_step.assign_add(1)
with tf.control_denpendices([add_global]):
    train_op = opt.minimise(loss)
'''
train_op = tf.train.GradientDescentOptimizer(learning_rate).minimise(
    loss,global_step=global_step)
with tf.Session() as sess:
    tf.global_variables_initializer().run()
    print(sess.run(learning_rate))
    for i in range(10):
        _= sess.run(train_op)
        print(rate)
~~~

### 笔记三十八 损失函数加上正则项

在损失函数加上正则项是防止过拟合的一个重要方法，tensorflow中对参数使用正则项分为两步：

1. 创建一个正则方法(函数/对象)

   - `tf.contrib.layers.l1_regularizer(scale,scope)`，返回执行`L1`正则化的函数
   - `tf.contrib.layers.l2_regularizer(scale,scope)`，返回执行`L2`正则化的函数
   - `tf.contrib.layers.sum_regularizer(scale,scope`，返回可执行多个正则化函数

2. 将这个正则方法应用到参数上

   使用`tf.contrib.layers.apply_regularization(regularizer, weights_list=None)`，函数返回一个标量Tensor，只需将这个正则项损失加到我们的损失函数上面

   - regularizer：就是上一步创建的正则化方法
   - weights_list：想执行正则化方法的参数列表







