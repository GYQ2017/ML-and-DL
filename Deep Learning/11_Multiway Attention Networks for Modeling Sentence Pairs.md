###  Multiway Attention Networks for Modeling Sentence Pairs

---

- 时间：2018-09-26
- 摘要：我们设计了四个注意力函数来匹配相应句子中的单词。然后，我们聚集来自每个函数的匹配信息，并组合来自所有函数的信息以获得最终表示。

#### 介绍

---

先前的工作使用具有注意力机制的神经网络，在该任务上显示了有效性。这些方法可以组织成两个框架：

1. 第一个框架是对句子对建模，对每个句子分别encoding(编码)，然后根据这两种表示做出决定。缺点是这两个句子在编码部分没有交互，一些方法运用注意机制来改善两个句子的互动。但是，注意力机制通常使用一个句子的表示来表示另一个句子，这仍然在句子层面起作用，但是缺乏单词层面的互动
2. 第二个框架基于匹配聚合框架，它在单词层面应用注意力机制来改善两个句子中单词之间的匹配。然后，匹配信息被聚集到句子级别以做出决定。这个框架支持单词级交互

本文，我们提出了一种用于句子对建模的多注意力网络，我们使用四种多注意力函数在单词层面上匹配两个句子，四种注意力函数包括串联注意力、双线性注意力、元素点积、两个向量的差来计算单词关系

我们的模型建立在匹配-聚合框架上，给定两个句子，用`BiRNN`进行编码以获得基于单词嵌入的两个句子中单词的上下文表示。然后我们应用上述四个注意函数在单词层面匹配两个句子，接下来，我们用两个步骤聚合来自多向注意函数的匹配信息。

1. 每个函数的匹配信息随着所有单词由`BiRNN`一起聚合
2. 通过应用注意力机制自适应的组合所有注意力函数的输出，然后应用另一个`BiRNN`来聚合混合匹配信息
3. 我们使用`attention-pooling`操作得到固定长度向量的匹配信息，并将其送到多层感知器中进行最终决策

#### 任务定义

---

我们表示每一个样例使用一个三元组(Q,P,y)，其中$Q=(q_1,...q_i,...q_N)$ 表示的是长度为N的句子，$P=(p_1,...,p_j,...p_M)$ 是长度为M的另一个句子，$y\in{Y}$是Q和P的关系标签

对于`释义识别任务`，Q和P是两个句子，$Y\in{\begin{Bmatrix}0,1\end{Bmatrix}}$ ，其中y=1意味着Q和P是同一个释义

对于`文本推理任务`，Q是前提句子，P是假设句子，$Y\in{\begin{Bmatrix}蕴涵,矛盾,中立\end{Bmatrix}}$ 

对于`答案选择任务`，Q是一个问题，P是答案，$Y\in{\begin{Bmatrix}0,1\end{Bmatrix}}$ ，其中y=1意味着P是正确答案

![11](/home/gaoyinquan/MS/Deep Learning/DL_Picture/11.png)

#### 方法

---

基于匹配-聚合的框架包含五部分：

1. 使用`BiRNN`获取两个句子的上下文单词表示
2. 对于来自Q和P的每一个单词对，使用四种多注意力函数获取四种匹配分数
3. 接下来，我们分两步将匹配信息和P中的单词聚合在一起。
4. 我们首先匹配每个注意力函数中的两个句子，然后组合来自所有函数的匹配信息，`BiRNN`用于在内部聚合和混合聚合中聚合匹配信息
5. 我们使用`attention-pooling` 来聚合P中的匹配信息以获得固定长度的向量，并应用多层感知器分类器进行最终决策

##### Multiway Matching

因此，在P的每个位置t，单词可以使用四个注意函数来匹配Q，以获得Q的相应加权和表示
$$
q_t^k = f_k(h^q,h_t^p,W_k)
$$
其中，$h_t^p$ 是P在位置t的表示，$h^q$ 是Q中所有位置的表示，$q_t^k$ 是基于参数$W_k$ 使用注意力函数$f_k$ 的Q的相应表示，$k\in(c,b,d,m)$ 代表四种注意力函数，其中$a_i^t$ 和$q^c_t$ 计算公式一样，只有$s^t_j$ 计算公式不同

`concat Attention` :
$$
s^t_j = v_c^Ttanh(W_c^1h_j^q+W_c^2h_t^p)
$$

$$
a_i^t = exp(s^t_i)/\sum^N_{j=1}exp(s^t_j)
$$

$$
q^c_t = \sum_{i=1}^Na_i^th^q_i
$$

`Bilinear Attention` :
$$
s^t_j = {h_j^q}^TW_bh^p_t
$$
`Dot Attention` :
$$
s^t_j = {v_d}^T tanh(W_d(h_j^q \odot h_t^p))
$$
`Minus Attention` :
$$
s^t_j = {v_m}^T tanh(W_m(h^q_j-h^p_t))
$$

##### Aggregation

我们分两步聚合来自多向注意力函数的匹配信息:

1. 内部聚合是将匹配信息与句子P中的单词一起聚合到每个注意力函数中，对于每个位置t，我们将P中的单词表示$h_t^p$与其对应的Q中的单词表示$q_t^k$连接起来，并添加一个门来确定连接表示的重要性，然后我们使用`BiGRU`通过P中的每个位置。我们使用`concat attention`作为样例：
   $$
   x_t^c = [q_t^c,h_t^p]
   $$

   $$
   g_t = sigmoid(W_gx_t^c)
   $$

   $$
   x_t^{c*} = g_t \odot x_t^c
   $$

   $$
   h_t^c = BiGRU(h_t^c,x_t^{c*})
   $$



2. 混合聚合是组合来自所有注意力函数的匹配信息，我们应用注意力机制将四个表示与参数 $v^a$ 自适应地组合作为输入，然后我们把 $x_t$ 送人`BiGRU`中去聚合混合匹配信息，以获得P中所有位置的$h^o$ 

$$
s_j = v^T tanh(W^1h_t^j+W^2v^a)(j=c,b,d,m)
$$

$$
a_i = exp(s_i)/\sum_{j=(c,b,d,m)}exp(s_j)
$$

$$
x_t = \sum_{j=(c,b,d,m)} a_i h_t^i
$$

##### Prediction Layer

在聚合来自多向匹配的信息后，我们将P中所有位置的结果表示转换成固定长度的向量，并将其输入分类器，以确定两个句子之间的关系。我们使用参数 $v^q$ 的`attention-pooling`来选择Q中的重要信息。
$$
s_j = v^T tanh(W^1_q h^q_j+W^2_q v^q)
$$

$$
a_i = exp(s_i)/\sum_{j=1}^N exp(s_j)
$$

$$
r^q = \sum_{i=1}^N a_i h^q_i
$$

然后，我们使用$r^q$ 来选择匹配向量中的信息
$$
s_j = v^T tanh(W^1_p h^o_j+W^2_p r^q)
$$

$$
a_i = exp(s_i)/\sum_{j=1}^M exp(s_j)
$$

$$
r^p = \sum_{i=1}^M a_i h^o_i
$$

最后，我们把$r^p$ 送到多层感知器中，用于对应任务中每个标签的概率$p_i$ ，对于所有任务，目标是最小化交叉熵:
$$
\zeta = -\sum_{i=1}^N [y_ilogp_i+(1-y_i)log(1-p_i)]
$$
