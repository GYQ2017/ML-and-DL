## 经典损失函数

- 时间：2018-07-11
- 摘要：主要介绍了交叉熵的由来、计算方式，以及Tensorflow两种不同场景的交叉熵实现方式

#### 交叉熵

神经网络解决多分类问题最常用的方法是设置 n 个输出节点，其中 n 为类别的个数。对于每一个样例，神经网络可以得到的一个 n 维数组作为输出结果。数组中的每一个维度（也就是每一个输出节点）对应一个类别。理想情况下，如果一个样本属于类别 k ，那么这个类别所对应的输出节点的输出值应该为 1, 而其他节点的输出值都为 0。以识别数字 1 为例，神经网络模型的输出结果越接近 [0,1,0,0,0,0,0,0,0,0] 越好。那么如何判断一个输出向量和期望的向量有多接近呢？交叉熵（cross entropy）是常用的评判方法之一。__交叉熵刻画了两个概率分布之间的距离，它是分类问题中使用比较广的一种损失函数__。

交叉熵是一个信息论中的概念，它原来是用来估算平均编码长度的。给定两个概率分布 p 和 q，通过 q 来表示 p 的交叉熵为：
$$
H(p,q)=-\sum_x p(x)\log q(x)
$$
注意交叉熵刻画的是两个概率分布之间的距离，然而神经网络的输出却不一定是一个概率分布。概率分布刻画了不同事件发生的概率，也就是说，任意事件发生的概率都在 0 和 1 之间，且总有某一个事件发生（概率的和为 1）。如果将分类问题中“一个样例属于某一个类别”看作一个概率事件，那么训练数据的正确答案就符合一个概率分布。因为事件“一个样例属于不正确的类别”的概率为 0，而“一个样例属于正确的类别”的概率为 1。如何将神经网络前向传播得到的结果也变成概率分布呢？Softmax 回归就是一个非常常用的方法。

Softmax 回归本身可以作为一个学习算法来优化分类结果，但在 TensorFlow 中，Softmax 回归的参数被去掉了，它只是一层额外的处理层，将神经网络的输出变成一个概率分布。假设原始的神经网络输出为 $y_1,y_2,...,y_n$，那么经过 Softmax 回归处理之后的输出为：
$$
softmax(y)_i=y'_i=\frac{e^{y_i}}{\sum_{j=1}^ne^{y_i}}
$$
从以上公式中可以看出，原始神经网络的输出被用作置信度来生成新的输出，而新的输出满足概率分布的所有要求。__这个新的输出可以理解为经过神经网络的推导，一个样例为不同类别的概率分别是多大__。这样就把神经网络的输出也变成了一个概率分布，从而可以通过交叉熵来计算预测的概率和真实答案的概率分布之间的距离了。

下面再给出两个具体样例来直观地说明通过交叉熵可以判断预测答案和真实答案之间的距离。假如有一个三分类问题，某个样例的正确答案是（1,0,0）。某模型经过 Softmax 回归之后的预测答案是（0.5,0.4,0.1），那么这个预测和正确答案之间的交叉熵为：
$$
H((1,0,0),(0.5,0.4,0.1))=-(1*\log0.5+0*\log0.4+0*\log0.1)\approx0.3
$$
 如果另外一个模型的预测是（0.8,0.1,0.1），那么这个预测和正确答案之间的交叉熵为：
$$
H((1,0,0),(0.8,0.1,0.1))=-(1*\log0.8+0*\log0.1+0*\log0.1)\approx0.1
$$
从直观上可以很容易地知道第二个预测答案要优于第一个。

因为交叉熵一般会与softmax回归一起使用，所以tensorflow进行了封装，并提供了函数，可直接通过以下代码来直接调用:

```python
import tensorflow as tf
loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_ ,logits=y)
```

 在只有__一个正确答案的分类问题中__，也提供了函数来进一步加速计算过程。

```python
import tensorflow as tf
loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_ ,logits=y)
```

---

参考文献：

- 《[TensorFlow：实战Google深度学习框架（第二版）](https://book.douban.com/subject/30137062/)》
-       [hemajun815](https://github.com/hemajun815/tutorial/blob/master/deep-learning/cross-entropy.md) 

